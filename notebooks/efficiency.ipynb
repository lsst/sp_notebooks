{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79802ee3-566a-4e44-ae5a-a17d9d418b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of day_obs for data query\n",
    "import rubin_nights.dayobs_utils as rn_dayobs\n",
    "\n",
    "day_obs = rn_dayobs.day_obs_str_to_int(rn_dayobs.today_day_obs())\n",
    "n_days = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec48311-6bdf-41ed-acd2-293b4817d383",
   "metadata": {},
   "source": [
    "# On-sky Efficiency {params.day_obs_min} to {params.day_obs_max}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some configuration for password/RPS tokenfiles\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Who is running the notebook? Some of us have preferences ..\n",
    "username = getpass.getuser()\n",
    "# Where is the notebook running? (RSPs are 'special')\n",
    "current_location = os.getenv(\"EXTERNAL_INSTANCE_URL\", \"\")\n",
    "\n",
    "# RUBIN_SIM_DATA_DIR at usdf\n",
    "if \"usdf\" in current_location:\n",
    "    os.environ[\"RUBIN_SIM_DATA_DIR\"] = \"/sdf/data/rubin/shared/rubin_sim_data\"\n",
    "\n",
    "# TOKEN CONFIGURATION\n",
    "if current_location != \"\":\n",
    "    # You are on an rsp.\n",
    "    # You should use the default RSP values, whether summit/base/USDF.\n",
    "    tokenfile = None\n",
    "    site = None\n",
    "# If you are outside of an RSP? - just use USDF and your own USDF-RSP token\n",
    "# See https://rsp.lsst.io/guides/auth/creating-user-tokens.html\n",
    "else:\n",
    "    # Substitute the location of your own tokenfile\n",
    "    tokenfile = os.getenv(\"ACCESS_TOKEN_FILE\", \"\")\n",
    "    site = os.getenv(\"DATA_SITE\", \"\")\n",
    "    if tokenfile == \"\":\n",
    "        # A very reasonable backup.\n",
    "        tokenfile = os.path.join(os.path.expanduser(\"~\"), \".lsst/usdf_rsp\")\n",
    "        site = \"usdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import warnings\n",
    "import copy\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import healpy as hp\n",
    "import matplotlib.pyplot as plt\n",
    "import colorcet as cc\n",
    "import skyproj\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from astropy.time import Time, TimeDelta\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "\n",
    "from rubin_scheduler.scheduler.utils import SchemaConverter\n",
    "from rubin_scheduler.site_models import Almanac\n",
    "from rubin_scheduler.scheduler.features import Conditions\n",
    "from rubin_scheduler.utils import (\n",
    "    ddf_locations,\n",
    "    angular_separation,\n",
    "    approx_ra_dec2_alt_az,\n",
    "    Site,\n",
    "    SURVEY_START_MJD,\n",
    ")\n",
    "\n",
    "import rubin_sim.maf as maf\n",
    "from rubin_sim.data import get_baseline\n",
    "\n",
    "from rubin_nights import connections\n",
    "import rubin_nights.dayobs_utils as rn_dayobs\n",
    "import rubin_nights.plot_utils as rn_plots\n",
    "import rubin_nights.augment_visits as augment_visits\n",
    "import rubin_nights.rubin_scheduler_addons as rn_sch\n",
    "import rubin_nights.rubin_sim_addons as rn_sim\n",
    "import rubin_nights.observatory_status as observatory_status\n",
    "import rubin_nights.scriptqueue as scriptqueue\n",
    "import rubin_nights.scriptqueue_formatting as scriptqueue_formatting\n",
    "import rubin_nights.targets_and_visits as targets_and_visits\n",
    "\n",
    "import importlib\n",
    "\n",
    "from lsst_survey_sim import lsst_support, simulate_lsst, plot\n",
    "\n",
    "band_colors = rn_plots.PlotStyles.band_colors\n",
    "logging.getLogger(\"rubin_nights\").setLevel(logging.INFO)\n",
    "\n",
    "# %load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up connections to data (will use consdb, exposure log and narrative log)\n",
    "endpoints = connections.get_clients(tokenfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b590a-74c6-4cb5-b553-45dcd0bb5faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_obs_max = day_obs\n",
    "day_obs_min = rn_dayobs.time_to_day_obs(\n",
    "    rn_dayobs.day_obs_to_time(day_obs_max) - TimeDelta(n_days, format=\"jd\")\n",
    ")\n",
    "day_obs_min = rn_dayobs.day_obs_str_to_int(day_obs_min)\n",
    "print(f\"Querying for lsstcam visits {day_obs_min} to {day_obs_max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Consdb data and add some flags for bad visits, filter changes, and calculate slew times and identify expected overheads vs. excess\n",
    "\n",
    "# We query all programs to get slewtimes and idle throughout entire night\n",
    "# But these are what will be considered \"science\"\n",
    "programs = [\"BLOCK-365\", \"BLOCK-407\", \"BLOCK-408\"]\n",
    "\n",
    "# Just a flag to make it clear if we're skipping retrieval from the consdb.\n",
    "# (this can make it much quicker to repeat this cell to tweak the inserts below)\n",
    "refresh_visits = True\n",
    "if refresh_visits:\n",
    "    skip_imgtypes = [\"bias\", \"flat\", \"dark\"]\n",
    "    query = (\n",
    "        \"select *, q.* from cdb_lsstcam.visit1 left join cdb_lsstcam.visit1_quicklook as q on visit1.visit_id = q.visit_id \"\n",
    "        f\"where visit1.day_obs >= {day_obs_min} and visit1.day_obs <= {day_obs_max} and img_type != 'bias' and img_type != 'flat' and img_type != 'dark'\"\n",
    "    )\n",
    "    visits = endpoints[\"consdb\"].query(query)\n",
    "    visits = augment_visits.augment_visits(visits, \"lsstcam\")\n",
    "    visits.reset_index(inplace=True)\n",
    "    visits.drop(\"index\", axis=1, inplace=True)\n",
    "    with warnings.catch_warnings(record=True) as w:\n",
    "        warnings.simplefilter(\"always\")\n",
    "        visits.to_hdf(\"v_now.h5\", key=\"visits\")\n",
    "else:\n",
    "    visits = pd.read_hdf(\"v_now.h5\")\n",
    "print(\n",
    "    \"all visits:\",\n",
    "    len(visits),\n",
    "    \"science visits:\",\n",
    "    len(visits.query(\"science_program in @programs\")),\n",
    ")\n",
    "\n",
    "\n",
    "if len(visits) > 0:\n",
    "\n",
    "    cols = [\"overhead\", \"fault_idle\", \"program_change\", \"filter_change\", \"bad_flag\"]\n",
    "    new_df = pd.DataFrame(\n",
    "        np.zeros((len(visits), len(cols))), columns=cols, index=visits.index\n",
    "    )\n",
    "    visits = visits.merge(new_df, right_index=True, left_index=True)\n",
    "\n",
    "    # Flag science_program changes\n",
    "    program_change = np.where(\n",
    "        (visits.science_program[:-1].values != visits.science_program[1:].values)\n",
    "    )[0]\n",
    "    program_change = program_change + 1\n",
    "    pmask = np.zeros(len(visits))\n",
    "    pmask[0] = 1\n",
    "    pmask[program_change] = 1\n",
    "    visits[\"program_change\"] = pmask\n",
    "\n",
    "    # Flag filter changes\n",
    "    filter_change = np.where(\n",
    "        (visits.band[:-1].values != visits.band[1:].values)\n",
    "        & (visits.day_obs[:-1].values == visits.day_obs[1:].values)\n",
    "    )[0]\n",
    "    filter_change = filter_change + 1\n",
    "    fmask = np.zeros(len(visits))\n",
    "    fmask[filter_change] = 1\n",
    "    visits[\"filter_change\"] = fmask\n",
    "\n",
    "    # calculate slew times and identify expected overheads\n",
    "    wait_before_slew = 1.6\n",
    "    settle = 1.5\n",
    "    max_scatter = 5.5\n",
    "    visits, slewing = rn_sch.add_model_slew_times(\n",
    "        visits,\n",
    "        endpoints[\"efd\"],\n",
    "        model_settle=wait_before_slew + settle,\n",
    "        dome_crawl=False,\n",
    "        slew_while_changing_filter=False,\n",
    "    )\n",
    "    valid_overhead = np.min(\n",
    "        [\n",
    "            np.where(np.isnan(visits.slew_model.values), 0, visits.slew_model.values)\n",
    "            + max_scatter,\n",
    "            visits.visit_gap.values,\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "    visits[\"overhead\"] = valid_overhead\n",
    "\n",
    "    # Need to remove faults for the first visit of the night or where there was a different program we didn't fetch\n",
    "    # (could skip *some* of this by fetching all visits, but might still have some missing due to flats?)\n",
    "    skipped_visits = np.concatenate(\n",
    "        [\n",
    "            np.array([0]),\n",
    "            np.where(visits.visit_id[:-1].values + 1 != visits.visit_id[1:].values)[0]\n",
    "            + 1,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    fault = visits.visit_gap - valid_overhead\n",
    "    fault[skipped_visits] = np.nan\n",
    "    visits[\"fault_idle\"] = fault\n",
    "\n",
    "    visits.loc[skipped_visits, \"model_gap\"] = np.nan\n",
    "\n",
    "    # Pull lsst-dm excluded visit list to flag bad visits\n",
    "    bad_visit_ids = augment_visits.fetch_excluded_visits(\"lsstcam\")\n",
    "    visits[\"bad_flag\"] = np.zeros(len(visits), int)\n",
    "    idx = visits.query(\"visit_id in @bad_visit_ids\").index\n",
    "    visits.loc[idx, \"bad_flag\"] = 1\n",
    "    # Also pull bad visit lists from exposure log\n",
    "    ee = endpoints[\"exposure_log\"].query_log(\n",
    "        rn_dayobs.day_obs_to_time(day_obs_min), rn_dayobs.day_obs_to_time(day_obs_max)\n",
    "    )\n",
    "\n",
    "    def make_visit_id(x):\n",
    "        return f\"{x.day_obs:d}{x.seq_num:05d}\"\n",
    "\n",
    "    exp_log_bad_visit_ids = (\n",
    "        ee.query(\"exposure_flag == 'junk'\").apply(make_visit_id, axis=1).values\n",
    "    )\n",
    "    idx = visits.query(\"visit_id in @exp_log_bad_visit_ids\").index\n",
    "    visits.loc[idx, \"bad_flag\"] = 1\n",
    "\n",
    "    sci = visits.query(\"science_program in @programs\")\n",
    "\n",
    "else:\n",
    "    print(\"Found no visits\")\n",
    "    print(\"The remainder of this notebook requires visits.\")\n",
    "\n",
    "## thoughts -- need to mark time associated with bad visits as fault somehow .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot expected slewtime vs. actual visit gap\n",
    "# Sometimes we want a subset of days ..\n",
    "\n",
    "print(\"For values attributable to SCIENCE visits only:\")\n",
    "\n",
    "dayobs = visits.day_obs.unique()\n",
    "\n",
    "q = visits.query(\"science_program in @programs and day_obs in @dayobs\")\n",
    "total_time = (q.shut_time.sum() + q.overhead.sum() + q.fault_idle.sum()) / 3600\n",
    "total_onsky = q.exp_time.sum() / 3600\n",
    "total_req = (q.shut_time.sum() + q.overhead.sum()) / 3600\n",
    "total_fault_idle = q.fault_idle.sum() / 3600\n",
    "dd = pd.DataFrame(\n",
    "    [total_time, total_onsky, total_req, total_fault_idle, len(q), len(q) * 30 / 3600],\n",
    "    index=[\n",
    "        \"time for visits\",\n",
    "        \"onsky exptime\",\n",
    "        \"onsky+overhead\",\n",
    "        \"fault+idle_sci\",\n",
    "        \"nvis\",\n",
    "        \"estimate time onsky\",\n",
    "    ],\n",
    "    columns=[\"all \" + \"_\".join(programs)],\n",
    ")\n",
    "display(dd.T)\n",
    "cols = [\n",
    "    \"visit_id\",\n",
    "    \"img_type\",\n",
    "    \"observation_reason\",\n",
    "    \"obs_start_mjd\",\n",
    "    \"band\",\n",
    "    \"s_ra\",\n",
    "    \"s_dec\",\n",
    "    \"sky_rotation\",\n",
    "    \"altitude\",\n",
    "    \"azimuth\",\n",
    "    \"physical_rotator_angle\",\n",
    "    \"clouds\",\n",
    "    \"fwhm_eff\",\n",
    "    \"filter_change\",\n",
    "    \"slew_distance\",\n",
    "    \"slew_model\",\n",
    "    \"visit_gap\",\n",
    "    \"model_gap\",\n",
    "]  # , 'overhead', 'fault_idle']\n",
    "# display(sci.query(\"model_gap > 2 and model_gap < 10\")[cols])\n",
    "\n",
    "print(\n",
    "    f\"Min/median/mean predicted overheads: {q.overhead.min():.2f} {q.overhead.median():.2f} {q.overhead.mean():.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Min/median/mean actual visit gaps: {q.visit_gap.min():.2f} {q.visit_gap.median():.2f} {q.visit_gap.mean():.2f}\"\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "ax = axes[0]\n",
    "ax.plot(q.visit_gap, q.slew_model, \"k.\")\n",
    "x = np.arange(0, 500, 1)\n",
    "ax.plot(x, x, alpha=0.3)\n",
    "ax.fill_betweenx(x1=x + max_scatter, x2=x, y=x, color=\"pink\", alpha=0.2)\n",
    "ax.set_xlim(0, 30)\n",
    "ax.set_ylim(0, 30)\n",
    "ax.grid(alpha=0.4)\n",
    "ax.set_xlabel(\"Visit gap (seconds)\")\n",
    "ax.set_ylabel(\"Slew model (seconds)\")\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(q.visit_gap, q.slew_model, \"k.\")\n",
    "x = np.arange(0, 500, 1)\n",
    "ax.plot(x, x, alpha=0.3)\n",
    "ax.fill_betweenx(x1=x + max_scatter, x2=x, y=x, color=\"pink\", alpha=0.2)\n",
    "# ax.set_xlim(0, 30)\n",
    "# ax.set_ylim(0, 30)\n",
    "ax.grid(alpha=0.4)\n",
    "ax.set_xlabel(\"Visit gap (seconds)\")\n",
    "ax.set_ylabel(\"Slew model (seconds)\")\n",
    "\n",
    "ax = axes[2]\n",
    "ax.plot(q.slew_distance, q.model_gap, \"k.\")\n",
    "ax.set_ylim(-10, 10)\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_xlabel(\"Slew distance (deg)\")\n",
    "_ = ax.set_ylabel(\"visit_gap - slew_model (seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize effect of current slew vs. \"ideal\" slew (40% + 3s settle)\n",
    "print(\"Slew effects only (no fault or idle)\")\n",
    "dayobs = visits.day_obs.unique()\n",
    "# dayobs = [20251108]\n",
    "q = visits.query(\"science_program in @programs and day_obs in @dayobs\")\n",
    "# Calculate \"open shutter fraction\" without faults\n",
    "slew_eff = (q.exp_time.sum()) / (q.dark_time.sum() + q.overhead.sum())\n",
    "ideal_eff = (q.exp_time.sum()) / (q.dark_time.sum() + q.slew_model_ideal.sum())\n",
    "print(f\"Slew efficiency factor: {slew_eff: 0.2f}\")\n",
    "print(f\"Ideal model efficiency equivalent: {ideal_eff: 0.2f}\")\n",
    "print(f\"Ratio: slew / ideal {slew_eff / ideal_eff :0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get narrative time lost logs\n",
    "time_lost_logs = endpoints[\"narrative_log\"].query_log(\n",
    "    rn_dayobs.day_obs_to_time(visits.day_obs.min()),\n",
    "    rn_dayobs.day_obs_to_time(visits.day_obs.max()),\n",
    "    {\"min_time_lost\": \"0.00001\"},\n",
    ")\n",
    "time_lost_logs = time_lost_logs.query(\"not component.str.contains('AuxTel')\")\n",
    "\n",
    "\n",
    "def time_to_day_obs(x):\n",
    "    return int(x.date_begin.split(\"T\")[0].replace(\"-\", \"\"))\n",
    "\n",
    "\n",
    "time_lost_logs[\"day_obs\"] = time_lost_logs.apply(time_to_day_obs, axis=1)\n",
    "log_fault = (\n",
    "    time_lost_logs.query(\"time_lost_type == 'fault'\")\n",
    "    .groupby(\"day_obs\")\n",
    "    .agg({\"time_lost\": \"sum\"})\n",
    "    .rename({\"time_lost\": \"log_fault\"}, axis=1)\n",
    ")\n",
    "log_weather = (\n",
    "    time_lost_logs.query(\"time_lost_type == 'weather'\")\n",
    "    .groupby(\"day_obs\")\n",
    "    .agg({\"time_lost\": \"sum\"})\n",
    "    .rename({\"time_lost\": \"log_weather\"}, axis=1)\n",
    ")\n",
    "log_lost = log_fault.merge(log_weather, how=\"outer\", on=\"day_obs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate time lost from visit gaps and overheads ..\n",
    "# Estimate some versions of effective system availability (per day)\n",
    "\n",
    "dd = {}\n",
    "for ddayobs in visits.day_obs.unique():\n",
    "    q = visits.query(\"day_obs == @ddayobs\")\n",
    "    nvis = len(q)\n",
    "    # Estimate of all fault/idle time - expect higher idle during other surveys\n",
    "    all_fault_idle = round(q.fault_idle.sum() / 60 / 60, 2)\n",
    "    # Estimate all fault/idle time - but only where visit_gap > 5 minutes\n",
    "    gap_fault_idle = round(q.query(\"visit_gap > 5*60\").fault_idle.sum() / 60 / 60, 2)\n",
    "    # -12 degree night length\n",
    "    sunset, sunrise = rn_dayobs.day_obs_sunset_sunrise(int(ddayobs), sun_alt=-12)\n",
    "    # Should also get dome open/close times?\n",
    "    # Estimate time from sunset to first science, and last science to sunrise.\n",
    "    qq = q.query(\"science_program in @programs\")\n",
    "    if len(qq) == 0:\n",
    "        twi_to_start = (sunrise.mjd - sunset.mjd) * 24 - gap_fault_idle\n",
    "        end_to_twi = 0\n",
    "    else:\n",
    "        twi_to_start = (qq.obs_start_mjd.min() - sunset.mjd) * 24\n",
    "        end_to_twi = (sunrise.mjd - qq.obs_end_mjd.max()) * 24\n",
    "    night_hours = (sunrise.mjd - sunset.mjd) * 24\n",
    "    # Estimate time spent in FBS compared to time expected for these visits in FBS\n",
    "    fbs = q.query(\"science_program in @programs\")\n",
    "    time_in_fbs = (fbs.obs_end_mjd.max() - fbs.obs_start_mjd.min()) * 24\n",
    "    time_predict_in_fbs = (fbs.dark_time.sum() + fbs.slew_model_ideal.sum()) / 60 / 60\n",
    "    dd[ddayobs] = [\n",
    "        ddayobs,\n",
    "        night_hours,\n",
    "        nvis,\n",
    "        twi_to_start,\n",
    "        end_to_twi,\n",
    "        time_in_fbs,\n",
    "        time_predict_in_fbs,\n",
    "        all_fault_idle,\n",
    "        gap_fault_idle,\n",
    "    ]\n",
    "\n",
    "dd = pd.DataFrame(\n",
    "    dd,\n",
    "    index=[\n",
    "        \"day_obs\",\n",
    "        \"night_hours\",\n",
    "        \"n_visits\",\n",
    "        \"twi_to_start\",\n",
    "        \"end_to_twi\",\n",
    "        \"time_in_fbs\",\n",
    "        \"time_predict_in_fbs\",\n",
    "        \"total_fault_idle\",\n",
    "        \"total_fault_idle_gap\",\n",
    "    ],\n",
    ").T\n",
    "dd = dd.merge(log_lost, how=\"outer\", on=\"day_obs\")\n",
    "dd[\"day_obs\"] = dd[\"day_obs\"].astype(int)\n",
    "dd.set_index(\"day_obs\", inplace=True)\n",
    "# Set time start/end of the night to the minimum of either 1.8 hours or the actual times\n",
    "# (use the min because sometimes we don't do FBS at all)\n",
    "missing_night_ends = np.min(\n",
    "    [\n",
    "        dd.night_hours.values - dd.twi_to_start.values - dd.end_to_twi.values,\n",
    "        np.ones(len(dd)) * 1.8,\n",
    "    ],\n",
    "    axis=0,\n",
    ")\n",
    "# Now calculate the fraction of the night which could have been available for science\n",
    "ratio_night_used = (dd.night_hours - missing_night_ends) / dd.night_hours\n",
    "# Estimate the fraction of the night available/run\n",
    "ratio = round(\n",
    "    (dd.night_hours - missing_night_ends - dd.total_fault_idle_gap) / (dd.night_hours),\n",
    "    2,\n",
    ")\n",
    "ratio = np.where(ratio <= 0, 0, ratio)\n",
    "dd[\"ratio_all_times\"] = ratio * slew_eff / ideal_eff\n",
    "dd[\"ratio_fbs_times\"] = round(\n",
    "    dd.time_predict_in_fbs / dd.time_in_fbs * ratio_night_used, 2\n",
    ")\n",
    "dd[\"ratio_sim_ref\"] = round((dd.night_hours - 0.37) / dd.night_hours, 2)\n",
    "\n",
    "non_ratio_cols = [c for c in dd if \"ratio\" not in c]\n",
    "dd.loc[\"total\", non_ratio_cols] = dd[non_ratio_cols].sum(axis=0)\n",
    "ratio_cols = [c for c in dd if \"ratio\" in c]\n",
    "dd.loc[\"total\", ratio_cols] = dd[ratio_cols].mean(axis=0)\n",
    "display(dd.round(2))\n",
    "\n",
    "print(\n",
    "    f\"fault+idle (hours) - total: {dd.total_fault_idle_gap.sum():.2f} mean: {np.nanmean(dd.total_fault_idle_gap):.2f}\"\n",
    ")\n",
    "\n",
    "# make an average ratio, with a fudge factor for time lost at ends of night\n",
    "ave_ratio = np.nanmean(\n",
    "    (dd.night_hours - dd.total_fault_idle_gap - 1.8) / dd.night_hours\n",
    ")\n",
    "ave_sim_ratio = np.mean((dd.night_hours - 0.37) / (dd.night_hours))\n",
    "print(f\"some kind of average ratio {ave_ratio:.2f} compare to {ave_sim_ratio:.2f}\")\n",
    "print(f\"Slew performance ratio {slew_eff / ideal_eff:.2f}\")\n",
    "print(f\"System availability estimate all: {np.nanmean(dd.ratio_all_times):.2f}\")\n",
    "print(f\"System availability estimate fbs: {np.nanmean(dd.ratio_fbs_times):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c77833-eaaa-41d5-a8f3-fb90b079b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Using baseline_v5.1.0_10yrs as the comparison\")\n",
    "fraction_wfd = 1692518.00 / 2075536.00\n",
    "fO_comparison = 762.00 / 825 * 0.9 / fraction_wfd\n",
    "print(f\"fraction WFD in this sim: {fraction_wfd:.2f}\")\n",
    "print(f\"median_nvis / 825 * 0.9 / fraction_wfd = {fO_comparison:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of above system availabity * fO_comparison info per night\n",
    "plt.figure(figsize=(12, 6))\n",
    "q = dd.iloc[:-1]\n",
    "x = np.arange(0, len(q))\n",
    "y = q[\"ratio_fbs_times\"] * fO_comparison\n",
    "plt.plot(x, y, marker=\".\", label=\"efficiency fbs visits\")\n",
    "y = q[\"ratio_all_times\"] * fO_comparison\n",
    "plt.plot(x, y, marker=\".\", label=\"efficiency all visits\")\n",
    "yy = np.nanmean(\n",
    "    np.concatenate(\n",
    "        [\n",
    "            q[\"ratio_all_times\"].values * fO_comparison,\n",
    "            q[\"ratio_fbs_times\"].values * fO_comparison,\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "plt.axhline(yy, color=\"gray\", linestyle=\":\", label=\"mean\")\n",
    "_ = plt.xticks(x, q.index, rotation=90)\n",
    "_ = plt.ylabel(\"SA*fO\")\n",
    "_ = plt.legend(loc=(1.01, 0.5))\n",
    "plt.grid(alpha=0.2)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "y = q[\"ratio_fbs_times\"]\n",
    "_ = plt.hist(y, bins=30, alpha=0.5, label=\"efficiency fbs visits\")\n",
    "y = q[\"ratio_all_times\"]\n",
    "_ = plt.hist(y, bins=30, alpha=0.5, label=\"efficiency all visits\")\n",
    "plt.xlabel(\"SA * fO\")\n",
    "plt.ylabel(\"Nnights\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ccc5eb-1c83-4c40-a037-d86714d2a6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c1791-f7df-4c73-9808-4198c31bfbda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
